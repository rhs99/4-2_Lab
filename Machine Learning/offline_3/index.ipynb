{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import math\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "alpha = 0.01\n",
    "INIT_W = 0.01\n",
    "\n",
    "\n",
    "dbg = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_mnist_train():\n",
    "    x, y = loadlocal_mnist(images_path='MNIST/train-images.idx3-ubyte',\n",
    "            labels_path='MNIST/train-labels.idx1-ubyte')\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_mnist_test():\n",
    "    x, y = loadlocal_mnist(images_path='MNIST/t10k-images.idx3-ubyte',\n",
    "            labels_path='MNIST/t10k-labels.idx1-ubyte')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z = wx + b\n",
    "\"\"\"\n",
    "class FullyConnected:\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.a = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.z = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, a):\n",
    "        if self.w is None or self.b is None:\n",
    "            self.w = np.random.randn(self.out_dim, a.shape[0])*INIT_W\n",
    "            self.b = np.zeros((self.out_dim, 1))\n",
    "\n",
    "        self.a = a\n",
    "        self.z = np.matmul(self.w, a) + self.b\n",
    "\n",
    "        if dbg:\n",
    "            print(\"fc_forward: \")\n",
    "            print(self.z.shape)\n",
    "\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m = dz.shape[1]\n",
    "        self.dw = np.matmul(dz, self.a.T)/m\n",
    "        self.db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "        da = np.matmul(self.w.T, dz)\n",
    "        self.w = self.w - alpha*self.dw\n",
    "        self.b = self.b - alpha*self.db\n",
    "        return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = ReLU(z)\n",
    "\"\"\"\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        r = np.maximum(0, z)\n",
    "        return r\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        dz = np.array(z, copy=True)\n",
    "        dz[dz<=0] = 0\n",
    "        dz[dz>0] = 1\n",
    "        return dz\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        self.a = self.relu(z)\n",
    "        if dbg:\n",
    "            print(\"relu_forward: \")\n",
    "            print(self.a.shape)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da):\n",
    "        dz = np.multiply(da, self.relu_derivative(self.z))\n",
    "        return dz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    m = y.shape[1]\n",
    "    logs = np.multiply(np.log(y_hat),y)\n",
    "    cost = - np.sum(logs) / m\n",
    "    return cost\n",
    "\n",
    "\"\"\"\n",
    "y_hat = e^z/sum(e^x)\n",
    "\"\"\"\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.out_dim = None\n",
    "        self.z = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.out_dim is None:\n",
    "            self.out_dim = z.shape[0]\n",
    "        self.z = z\n",
    "        self.y_hat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        if dbg:\n",
    "            print(\"soft_forward: \")\n",
    "            print(self.y_hat.shape)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y):\n",
    "        dz = self.y_hat - y\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Convolutional:\n",
    "    def __init__(self, number_of_filters, filter_dim, stride=1, padding = 0):\n",
    "        self.number_of_filters = number_of_filters\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.a_prev = None\n",
    "\n",
    "    @staticmethod\n",
    "    def add_padd(x, pad):\n",
    "        x_pad = np.pad(x, ((0,0), (pad, pad), (pad, pad), (0,0)), mode='constant', constant_values = (0,0))\n",
    "        return x_pad\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_single_step(a_slice_prev, w, b):\n",
    "        s = np.multiply(a_slice_prev, w)\n",
    "        z = np.sum(s)\n",
    "        z = z + float(b)\n",
    "        return z\n",
    "\n",
    "    def convolve(self, x, y, b, pad,m, stride, nh, nw, nc, f):\n",
    "        x_p = self.add_padd(x, pad)\n",
    "        z = np.zeros([m, nh, nw, nc])\n",
    "        for i in range(m):\n",
    "            x_cur = x_p[i]\n",
    "            for h in range(nh):\n",
    "                v_shuru = stride * h\n",
    "                v_shesh = v_shuru + f\n",
    "                for w in range(nw):\n",
    "                    h_shuru = stride * w\n",
    "                    h_shesh = h_shuru + f\n",
    "                    for c in range(nc):\n",
    "                        x_slice = x_cur[ v_shuru:v_shesh, h_shuru:h_shesh, :]\n",
    "                        weights = y[:, :, :, c]\n",
    "                        biases = b[:, :, :, c]\n",
    "                        z[i, h, w, c] = self.conv_single_step(x_slice, weights, biases)\n",
    "        return z\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, nh_prev, nw_prev, nc_prev) = a_prev.shape\n",
    "\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(self.filter_dim, self.filter_dim, nc_prev, self.number_of_filters)*INIT_W\n",
    "            self.b = np.zeros((1, 1, 1, self.number_of_filters))\n",
    "\n",
    "        (fh, fw, nc_prev, nc) = self.w.shape\n",
    "\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "\n",
    "        nh = int(int(nh_prev + 2*pad - fh)/stride + 1)\n",
    "        nw = int(int(nw_prev + 2*pad - fw)/stride + 1)\n",
    "\n",
    "        z = self.convolve(a_prev,self.w,self.b,pad,m,self.stride,nh,nw,nc,fh)\n",
    "\n",
    "        assert(z.shape == (m, nh, nw, nc))\n",
    "        if dbg:\n",
    "            print(\"conv_forward: \")\n",
    "            print(z.shape)\n",
    "        return z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        (m, nh_prev, nw_prev, nc_prev) = self.a_prev.shape\n",
    "        (fh, fw, nc_prev, nc) = self.w.shape\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        (m, nh, nw, nc) = dz.shape\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "        dw = np.zeros(self.w.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "        a_prev_pad = self.add_padd(self.a_prev, pad)\n",
    "        da_prev_pad = self.add_padd(da_prev, pad)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_pad_cur = a_prev_pad[i]\n",
    "            da_prev_pad_cur = da_prev_pad[i]\n",
    "            for h in range(nh):\n",
    "                for w in range(nw):\n",
    "                    for c in range(nc):\n",
    "                        v_shuru = stride * h\n",
    "                        v_shesh = v_shuru + fh\n",
    "                        h_shuru = stride * w\n",
    "                        h_shesh = h_shuru + fw\n",
    "\n",
    "                        a_slice = a_prev_pad_cur[v_shuru:v_shesh,h_shuru:h_shesh,:]\n",
    "                        da_prev_pad_cur[v_shuru:v_shesh, h_shuru:h_shesh, :] += self.w[:,:,:,c] * dz[i, h, w, c]\n",
    "                        dw[:,:,:,c] += a_slice * dz[i, h, w, c]\n",
    "                        db[:,:,:,c] += dz[i, h, w, c]\n",
    "\n",
    "            if pad > 0:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[pad:-pad, pad:-pad, :]\n",
    "            else:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[:, :, :]\n",
    "\n",
    "        assert(da_prev.shape == (m, nh_prev, nw_prev, nc_prev))\n",
    "\n",
    "        self.w = self.w - alpha*dw\n",
    "        self.b = self.b - alpha*db\n",
    "\n",
    "        return da_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, nh_prev, nw_prev, nc_prev) = a_prev.shape\n",
    "        f = self.filter_dim\n",
    "        stride = self.stride\n",
    "        nh = int(1 + (nh_prev - f) / stride)\n",
    "        nw = int(1 + (nw_prev - f) / stride)\n",
    "        nc = nc_prev\n",
    "\n",
    "        a = np.zeros((m, nh, nw, nc))\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_cur = a_prev[i]\n",
    "            for h in range(nh):\n",
    "                v_shuru = stride * h\n",
    "                v_shesh = v_shuru + f\n",
    "                for w in range(nw):\n",
    "                    h_shuru = stride * w\n",
    "                    h_shesh = h_shuru + f\n",
    "                    for c in range (nc):\n",
    "                        a_prev_cur_slice = a_prev_cur[v_shuru:v_shesh,h_shuru:h_shesh,c]\n",
    "                        a[i, h, w, c] = np.max(a_prev_cur_slice)\n",
    "\n",
    "        assert(a.shape == (m, nh, nw, nc))\n",
    "        if dbg:\n",
    "            print(\"pool_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mask_from_window(x):\n",
    "        mask = (x == np.max(x))\n",
    "        return mask\n",
    "\n",
    "    def backward(self, da):\n",
    "        stride = self.stride\n",
    "        f = self.filter_dim\n",
    "        (m, nh, nw, nc) = da.shape\n",
    "\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev = self.a_prev[i,:,:,:]\n",
    "\n",
    "            for h in range(nh):\n",
    "                for w in range(nw):\n",
    "                    for c in range(nc):\n",
    "                        v_shuru  = h * stride\n",
    "                        v_shesh    = v_shuru + f\n",
    "                        h_shuru = w * stride\n",
    "                        h_shesh   = h_shuru + f\n",
    "\n",
    "                        a_prev_slice = a_prev[ v_shuru:v_shesh, h_shuru:h_shesh, c ]\n",
    "                        mask = self.create_mask_from_window( a_prev_slice )\n",
    "                        da_prev[i, v_shuru:v_shesh, h_shuru:h_shesh, c] += mask * da[i, h, w, c]\n",
    "\n",
    "        assert(da_prev.shape == self.a_prev.shape)\n",
    "        return da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flattening:\n",
    "    def __init__(self):\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        m = a_prev.shape[0]\n",
    "        a = list()\n",
    "        for i in range(m):\n",
    "            a.append(np.ravel(a_prev[i,:,:,:]))\n",
    "        a = np.array(a)\n",
    "        a = a.T\n",
    "        if dbg:\n",
    "            print(\"flatten_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "    \"\"\"\n",
    "    (m,h,w,c) -> h*w*c,m -> m,h*w*c ->\n",
    "\n",
    "    \"\"\"\n",
    "    def backward(self, da):\n",
    "        da = da.T\n",
    "        da_prev = da.reshape(self.a_prev.shape)\n",
    "        return da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.306200911979168\n",
      "2.3062534136246473\n",
      "2.2993054118411567\n",
      "2.3115523834279372\n",
      "2.301056512761414\n",
      "2.303533143265494\n",
      "2.302112543210028\n",
      "2.3086776436688576\n",
      "2.300058060909449\n",
      "2.2986692028910003\n",
      "2.298737534933602\n",
      "2.293263062168191\n",
      "2.29535724486525\n",
      "2.2921234305608813\n",
      "2.3048397285165168\n",
      "2.296534176591899\n",
      "2.3036391722496448\n",
      "2.288997770207111\n",
      "2.297925793930588\n",
      "2.2714381895795617\n",
      "2.309068079126524\n",
      "2.2819583819208145\n",
      "2.2429943692975813\n",
      "2.271337940033372\n",
      "2.1983707395542864\n",
      "2.461364606902733\n",
      "2.3262946978383416\n",
      "2.3163156250206782\n",
      "2.3078574802944005\n",
      "2.304801877812649\n",
      "2.2942334289863275\n",
      "2.2793805812210444\n",
      "2.2883576903245917\n",
      "2.271159645724143\n",
      "2.3168429782737148\n",
      "2.2898492585117056\n",
      "2.286578841671349\n",
      "2.2770927016784377\n",
      "2.2575658466573048\n",
      "2.2561338883219757\n",
      "2.293398577351996\n",
      "2.160491730277289\n",
      "2.148059046707667\n",
      "2.3212643633502785\n",
      "2.221671254890685\n",
      "2.3510532281001932\n",
      "2.1442892052232656\n",
      "2.405418362928246\n",
      "2.188232200208507\n",
      "2.1297187739140178\n",
      "2.1809676066177\n",
      "2.230839856150654\n",
      "2.1188955994534457\n",
      "2.3040099671569103\n",
      "2.1598108043824764\n",
      "1.9380116803318181\n",
      "1.984147794456386\n",
      "2.032444520511633\n",
      "1.596708955504773\n",
      "1.6998035508192224\n",
      "1.8026872871057087\n",
      "2.3169361831904394\n",
      "2.0039805741040966\n",
      "1.944858056938895\n",
      "1.9118723959898951\n",
      "1.7056645211473822\n",
      "1.8695680781207635\n",
      "1.7100567384686767\n",
      "1.5441690470285725\n",
      "1.648023065531951\n",
      "1.3485293874453248\n",
      "1.4755312359305925\n",
      "1.9724146827690703\n",
      "1.7887802626114762\n",
      "1.144082553235056\n",
      "0.8776799908181122\n",
      "1.341014941580203\n",
      "1.4309328677457085\n",
      "1.4998898589883083\n",
      "3.084136959211281\n",
      "2.230677023418475\n",
      "1.783945795035876\n",
      "1.7744820332072178\n",
      "2.3196895981750223\n",
      "1.5011224602513882\n",
      "0.875815964345407\n",
      "1.1385760172335764\n",
      "1.2174114918588672\n",
      "1.5582156946600736\n",
      "1.073166445740755\n",
      "0.8905786512393103\n",
      "1.4807409549313255\n",
      "2.0189674013890673\n",
      "0.9808807461507014\n",
      "1.1468307299107816\n",
      "1.0396759443276509\n",
      "1.1275801276147117\n",
      "2.4024565090887493\n",
      "1.1659765409287832\n",
      "2.8575034651465523\n",
      "1.7829076655340241\n",
      "1.9162161406285925\n",
      "0.8443492230221963\n",
      "1.5656432566887724\n",
      "1.2715865300809708\n",
      "1.3220834566733137\n",
      "2.408169944584892\n",
      "1.0027120709416655\n",
      "1.5900562535671252\n",
      "1.5981019246985033\n",
      "0.8378662866511316\n",
      "0.9393316352406286\n",
      "0.5082592855220414\n",
      "1.0295087352430554\n",
      "0.8027776039910247\n",
      "0.6649784937054\n",
      "1.6068291944231567\n",
      "1.505366361769839\n",
      "0.5400106951729533\n",
      "0.8151839060767658\n",
      "0.6470243745970012\n",
      "0.44706628027657597\n",
      "0.2927347062095727\n",
      "0.21065276236730418\n",
      "0.025379853586477923\n",
      "0.9265737647532392\n",
      "1.7011772369115739\n",
      "0.6901492792040302\n",
      "2.4599306500074265\n",
      "1.4471340818622203\n",
      "1.1702721180890054\n",
      "0.9521940160493718\n",
      "1.1593067647994715\n",
      "1.1294377908199498\n",
      "0.7101021044127339\n",
      "0.40012745740988986\n",
      "1.2051944301259199\n",
      "0.8184877784292597\n",
      "0.5493137817684361\n",
      "1.5203457845009578\n",
      "0.6125219502319676\n",
      "0.10294799816677917\n",
      "0.6527840340712733\n",
      "0.6156839797201583\n",
      "1.2668555086404492\n",
      "1.2378524359105039\n",
      "0.7333546517143883\n",
      "0.6401756393995834\n",
      "0.6955552205971893\n",
      "0.9650456017708784\n",
      "0.9487945907305108\n",
      "1.1572566243695828\n",
      "0.648036338384775\n",
      "2.0520511552228866\n",
      "0.7868004030749055\n",
      "0.8303425097356325\n",
      "0.8462734086074806\n",
      "0.32400304169107874\n",
      "0.5321751596492308\n",
      "0.4151875974943892\n",
      "0.44868138695610327\n",
      "0.2502425551612777\n",
      "0.34770766986865326\n",
      "0.33626912789397617\n",
      "0.3682933791940692\n",
      "0.03773178298294467\n",
      "0.5128377864719342\n",
      "0.8915037538816538\n",
      "2.131337769275992\n",
      "1.2181871909584199\n",
      "0.8782338302593473\n",
      "0.10525819040021549\n",
      "0.7535508025219017\n",
      "0.40960257241664355\n",
      "0.456148586722831\n",
      "0.4165176529750223\n",
      "0.49049388304865377\n",
      "0.4652682945528028\n",
      "0.9442221109068871\n",
      "0.507686897868884\n",
      "0.6237969504097424\n",
      "0.25411165017750864\n",
      "0.36786390890880455\n",
      "1.908640089520965\n",
      "0.42688667195961055\n",
      "1.2965068789924854\n",
      "0.16975360000239986\n",
      "1.353787101562147\n",
      "0.805245663319227\n",
      "0.26475817511054833\n",
      "0.10090805043860954\n",
      "1.27586075413141\n",
      "0.6390551929565721\n",
      "0.2734539820357809\n",
      "0.6728287542333702\n",
      "0.12484509334875768\n",
      "0.3116935004448779\n",
      "0.9048883459790712\n",
      "1.9766076601721807\n",
      "1.7206173412174146\n",
      "2.018916500929106\n",
      "0.6955168861284379\n",
      "0.2693222731601076\n",
      "0.6490278147757418\n",
      "0.8052579372393875\n",
      "1.2120730842172946\n",
      "1.7081486367269103\n",
      "0.6619781844811425\n",
      "0.8644292257964235\n",
      "0.921466317944288\n",
      "0.6295148744270339\n",
      "0.23331298533720166\n",
      "0.2290596997973727\n",
      "0.04862931264935385\n",
      "0.06129735545105852\n",
      "0.018600664892954748\n",
      "1.1436304497514516\n",
      "0.24792217582934623\n",
      "0.05912920884745988\n",
      "0.17879737700773976\n",
      "0.6502072372500968\n",
      "0.3067793641014634\n",
      "0.047369269229781574\n",
      "0.04944890071062681\n",
      "0.009531075070390436\n",
      "0.16562161714765217\n",
      "1.625333682569909\n",
      "0.4250273932754203\n",
      "0.9862038525939021\n",
      "1.0474092925159628\n",
      "0.7727056978401976\n",
      "0.2999928440868238\n",
      "0.5625223998243815\n",
      "0.7399623025680091\n",
      "0.7764099926482306\n",
      "0.6762425326708765\n",
      "0.8010348401778918\n",
      "2.3383886734847095\n",
      "1.1388098555196653\n",
      "1.2117301200869932\n",
      "0.3907280838334489\n",
      "0.12551429735601996\n",
      "0.3685436047666344\n",
      "0.36833724039783344\n",
      "0.603584626035286\n",
      "0.7970796783492121\n",
      "0.28339791180085344\n",
      "0.21372607357200119\n",
      "0.5591879354627342\n",
      "0.19355571847769157\n",
      "0.5783097936669606\n",
      "0.7990911411848411\n",
      "0.6064316758968916\n",
      "1.0441389610985312\n",
      "0.1842404711438943\n",
      "0.521586377323584\n",
      "0.33194117794334355\n",
      "1.1728965852312385\n",
      "0.4089509153176376\n",
      "0.06103422092355724\n",
      "0.1737648626419419\n",
      "0.3135979982581881\n",
      "0.63602927860354\n",
      "0.28757590537394573\n",
      "0.46128492296675594\n",
      "0.024291537120175878\n",
      "0.16468181477106172\n",
      "0.3776859613252076\n",
      "1.464185710896539\n",
      "1.140020886543948\n",
      "0.8828874988569204\n",
      "0.17354429734311194\n",
      "0.531283248234438\n",
      "0.0809470123937137\n",
      "0.1537186098704419\n",
      "0.4416426986329016\n",
      "0.07290589292289787\n",
      "0.4038182337172268\n",
      "0.38332678952564625\n",
      "0.09688739690834239\n",
      "0.6907477320332159\n",
      "0.029229233544392434\n",
      "0.3931652215686768\n",
      "0.252178721028856\n",
      "1.2557123238768422\n",
      "1.9551510251130508\n",
      "0.21354197387888035\n",
      "0.3603469954528008\n",
      "0.2892971738496736\n",
      "0.020010020081806468\n",
      "0.015248711549877225\n",
      "0.44559436967363053\n",
      "0.29783740173764134\n",
      "0.13071427923831896\n",
      "0.36239250990046257\n",
      "0.03012241589369811\n",
      "0.4478803583586073\n",
      "0.443393720771175\n",
      "1.0732293456892112\n",
      "0.3943934832751354\n",
      "0.8923718378262414\n",
      "0.14447091177301358\n",
      "0.1295150150843561\n",
      "0.0890426586010888\n",
      "0.8212690573005931\n",
      "0.923049231115297\n",
      "1.4244104860360316\n",
      "0.31139544149574927\n",
      "1.3313871271335072\n",
      "0.670012478134704\n",
      "0.36145172628903277\n",
      "0.06385647727203256\n",
      "0.01949075859440068\n",
      "0.0801198585728347\n",
      "0.09026194698939964\n",
      "0.007542263873224779\n",
      "1.0258748499087584\n",
      "0.40302063330730753\n",
      "0.05722151285762793\n",
      "0.019119158109318597\n",
      "0.2575919753906494\n",
      "0.20408449602844989\n",
      "0.007659197477522297\n",
      "0.010950042978942085\n",
      "0.20384588032552817\n",
      "0.014996049474164469\n",
      "1.1711596186164837\n",
      "0.03872473033618852\n",
      "0.9518082621413159\n",
      "0.7116291461290506\n",
      "0.969570814897726\n",
      "0.31044005978216427\n",
      "0.8621934103277592\n",
      "0.3634183506776799\n",
      "0.02375475956464666\n",
      "0.11676219628134313\n",
      "0.5496521768872387\n",
      "0.567514353566648\n",
      "0.08271227829197231\n",
      "0.3551071730243571\n",
      "0.07748695219299342\n",
      "0.02916810213077517\n",
      "1.094631923216401\n",
      "0.1185091433448121\n",
      "0.4800556382859017\n",
      "0.19609205003159969\n",
      "0.28296515268343286\n",
      "0.04409628582310939\n",
      "0.8362160854946561\n",
      "0.1008302562958078\n",
      "0.4359118699139226\n",
      "0.5534172115251075\n",
      "0.4233917011489076\n",
      "0.5797154171822414\n",
      "0.05052792171381372\n",
      "0.045847217563454785\n",
      "0.487499227905547\n",
      "0.032759375293307844\n",
      "0.4984508206140495\n",
      "0.0493457486671226\n",
      "0.10042372939096325\n",
      "0.19643150806653625\n",
      "0.03437580063731823\n",
      "0.043147120907044464\n",
      "0.013991418441976506\n",
      "0.0008992066638906099\n",
      "0.04309436382505856\n",
      "0.020684555877308865\n",
      "0.6711651539660084\n",
      "1.2823249645931418\n",
      "0.5991804496533609\n",
      "0.007766227026113711\n",
      "0.8567164857522112\n",
      "1.0697227877001168\n",
      "0.14970700418571636\n",
      "0.29894136269364835\n",
      "0.1702892654291915\n",
      "0.6072188143137905\n",
      "0.901711248066791\n",
      "0.04355656666983655\n",
      "0.8396768873845051\n",
      "0.09184323186849537\n",
      "0.6023427250722457\n",
      "0.778581918001319\n",
      "0.22614665482253452\n",
      "0.7709555999348213\n",
      "0.05071047005513448\n",
      "0.08111047571256962\n",
      "0.09335890733400302\n",
      "0.0040026356900204245\n",
      "0.031216351329518048\n",
      "0.29376313949378163\n",
      "0.8353954266075847\n",
      "0.08341975930081376\n",
      "0.3439016266641701\n",
      "0.02733895705889965\n",
      "0.05238672008249384\n",
      "0.4350369605623291\n",
      "0.2103073075222662\n",
      "0.5106340569206004\n",
      "0.20954351260629558\n",
      "0.02635963161314231\n",
      "0.06778635339843088\n",
      "0.17774840742037434\n",
      "0.2966232703185162\n",
      "0.10912347574335793\n",
      "0.48297179816852953\n",
      "0.30227780405555593\n",
      "0.3502245033392448\n",
      "0.6761666377590678\n",
      "0.3775347175067406\n",
      "1.1558212469718894\n",
      "0.022978006858050817\n",
      "0.07628749401542058\n",
      "0.05637422832674208\n",
      "0.021955345734627724\n",
      "1.0071749861090329\n",
      "0.2747510163554018\n",
      "0.019144931737444675\n",
      "0.023796823315533695\n",
      "0.10156197951120871\n",
      "0.01629225838386118\n",
      "0.4383018433251201\n",
      "0.01585013209121418\n",
      "0.17212538442042163\n",
      "0.41910742776786575\n",
      "3.2955432714894406\n",
      "1.095936913432856\n",
      "0.543755854328777\n",
      "0.2049098185873072\n",
      "1.44167613533539\n",
      "0.23033126378816754\n",
      "0.9539876790988687\n",
      "0.8302980786700399\n",
      "0.08319090992918851\n",
      "0.06322368439341565\n",
      "0.485594130263197\n",
      "0.3481234124784893\n",
      "0.014783469888725498\n",
      "0.4672576238564815\n",
      "0.7141283444693654\n",
      "0.014909518192439922\n",
      "0.4966489672242182\n",
      "0.1257959581199869\n",
      "0.5234751894812193\n",
      "0.1772481596172541\n",
      "0.2842223931396971\n",
      "0.04482522388207766\n",
      "0.9195640535594085\n",
      "0.0214262302648714\n",
      "1.0638602948774556\n",
      "0.6024937609771366\n",
      "0.09834685001194657\n",
      "0.08792304690462478\n",
      "0.02267594676552489\n",
      "0.03437922481271204\n",
      "0.007486352708140716\n",
      "0.011965558318177916\n",
      "0.005440441608403653\n",
      "0.0013548916034862165\n",
      "0.14375429216841862\n",
      "0.3550765486083626\n",
      "0.24842887659187246\n",
      "0.19587738617162534\n",
      "0.02323596150956616\n",
      "0.00109599352304014\n",
      "0.022981129789700357\n",
      "0.0004139665370163352\n",
      "0.20265774839106507\n",
      "0.6012776255512229\n",
      "0.5030452901013093\n",
      "0.04390281475963667\n",
      "0.6115579481246824\n",
      "0.016934567870916184\n",
      "0.11724680047108212\n",
      "0.4573740516417132\n",
      "0.008646844264004677\n",
      "0.16617223766680428\n",
      "0.3884841526042204\n",
      "0.08815295325775716\n",
      "0.3463970817834525\n",
      "0.010851405090608874\n",
      "0.32005871451773327\n",
      "0.2319078837908711\n",
      "0.09289520865512382\n",
      "0.4845384723632704\n",
      "0.05213668894202459\n",
      "0.07972758725717545\n",
      "0.017274665957912265\n",
      "0.005019413668105694\n",
      "0.005869762862998531\n",
      "0.008900867670998779\n",
      "0.038464395201387946\n",
      "0.06564042324451042\n",
      "0.01511794054040658\n",
      "0.0007082704585592217\n",
      "0.050206035269035124\n",
      "0.0214724135095686\n",
      "0.4290069278485634\n",
      "0.022903280643352734\n",
      "0.495466747607104\n",
      "0.01285612498085183\n",
      "0.013719149705136666\n",
      "0.044976247841627334\n",
      "0.05788589459013035\n",
      "0.022326788130270902\n",
      "0.21362406137082474\n",
      "0.003175826480084038\n",
      "0.008135486086518597\n",
      "0.3216026269947502\n",
      "0.011213186267112318\n",
      "0.09371982560129037\n",
      "0.006226149402767439\n",
      "0.3998696313422373\n",
      "0.34355592039663196\n",
      "3.0119934689467494\n",
      "1.2153576183231833\n",
      "0.2889563951153215\n",
      "0.005278977883464751\n",
      "0.10108990407573619\n",
      "0.019216549359145452\n",
      "0.1502393844568713\n",
      "0.0023260312245293375\n",
      "0.004805916054853356\n",
      "0.0019916082814194656\n",
      "0.016887748300086075\n",
      "0.5582014713501443\n",
      "0.03772539143895171\n",
      "0.029491369201946095\n",
      "0.37639419132733676\n",
      "0.013710804587064659\n",
      "0.0012301130797254527\n",
      "0.5279499998269982\n",
      "0.16004097561083372\n",
      "1.1096861257586714\n",
      "0.036113774178905704\n",
      "0.6492540196939675\n",
      "0.01866904506951409\n",
      "0.025336967769883194\n",
      "0.23005085693824728\n",
      "1.2315948640152445\n",
      "0.7695171039210925\n",
      "0.5562353783659162\n",
      "0.15883670832344246\n",
      "0.3384858039964247\n",
      "0.3756240032010481\n",
      "0.19581835402148015\n",
      "0.02547701161736817\n",
      "1.0659603046671522\n",
      "0.02874181711846645\n",
      "0.7160283769503858\n",
      "0.0716114950375387\n",
      "0.10299159451828321\n",
      "0.8385322987013172\n",
      "0.004928092378956734\n",
      "0.06985402968207868\n",
      "0.5361221934842427\n",
      "0.07702239587843397\n",
      "0.007782152501117702\n",
      "0.8990725886693628\n",
      "0.14603718385933281\n",
      "0.6398655746822549\n",
      "0.008422241283465135\n",
      "0.01672902397708943\n",
      "0.0021422982804060312\n",
      "0.004269360299687013\n",
      "0.014020416084713373\n",
      "0.0027914616257146314\n",
      "0.8910234949077243\n",
      "0.13050688751344613\n",
      "0.11908218294332143\n",
      "0.010608962352579266\n",
      "0.2094505361531161\n",
      "0.0010875738806360592\n",
      "0.09999661041877397\n",
      "0.08376793766341725\n",
      "0.055542491474367385\n",
      "0.028973894056446643\n",
      "0.04207592635788872\n",
      "0.012193409000476626\n",
      "0.23049959273147858\n",
      "0.010499263080920113\n",
      "0.14109191615501085\n",
      "0.4231275098437325\n",
      "0.050959585209349775\n",
      "0.010766050697113337\n",
      "0.4249985939576574\n",
      "0.001449488166410844\n",
      "0.09003664300613859\n",
      "0.02099654041228487\n",
      "0.25345617184376174\n",
      "0.3815265647456945\n",
      "0.5319170103303233\n",
      "0.734716464390395\n",
      "0.46470815470540294\n",
      "0.08758138764723791\n",
      "0.2403668017356039\n",
      "0.010069067127931604\n",
      "0.039394352017996195\n",
      "0.0009823630952225594\n",
      "0.03069021414551367\n",
      "0.054706027894969055\n",
      "0.009659401922220187\n",
      "0.035470503163936434\n",
      "0.04342089557733679\n",
      "0.9525739580133245\n",
      "0.4089431972884995\n",
      "1.9520075645616413\n",
      "0.9456250883456478\n",
      "0.6065894730231828\n",
      "0.04579984421660392\n",
      "0.017939921466851007\n",
      "0.014428505402899755\n",
      "0.00261905936680968\n",
      "0.03157530093349044\n",
      "0.0003308197260382706\n",
      "0.44972741111984194\n",
      "0.07953356020492644\n",
      "0.002578138933362871\n",
      "0.01864100763304226\n",
      "0.13973257902328617\n",
      "0.5807009245005142\n",
      "0.0031599117841864532\n",
      "0.10611135575588179\n",
      "0.008273748670101146\n",
      "0.04842341983025385\n",
      "0.4792602487580959\n",
      "0.1475019355253546\n",
      "0.39085226946037144\n",
      "0.6954025733374396\n",
      "0.2861746241433759\n",
      "0.0029814147533036533\n",
      "0.38047317683261933\n",
      "0.08353659979081238\n",
      "0.04949120896954348\n",
      "0.016420217035553762\n",
      "0.33155042142798635\n",
      "0.1709489330372722\n",
      "0.04511902817524058\n",
      "0.05364371824150173\n",
      "0.01533177040660664\n",
      "0.0005303442101685159\n",
      "0.30952191802423884\n",
      "0.08136060078649819\n",
      "0.46438715608612346\n",
      "0.4488237819826869\n",
      "0.0013119543398504633\n",
      "0.06118872954039084\n",
      "0.8059760430370323\n",
      "0.18948121912319688\n",
      "0.6524567995349658\n",
      "0.045519750095479115\n",
      "0.09484298906441858\n",
      "0.05525883340940514\n",
      "0.0013837106910936567\n",
      "0.1459220118848974\n",
      "0.006441517328752931\n",
      "0.05002076275571472\n",
      "9.2358839219393e-05\n",
      "5.81416843896156e-05\n",
      "0.2980911680393363\n",
      "0.14952230534453503\n",
      "0.004656239814582781\n",
      "0.09380712139257262\n",
      "0.0018117724908986918\n",
      "7.413178116604138e-06\n",
      "0.044034330991122216\n",
      "0.00015280543011425806\n",
      "0.27991526358667296\n",
      "0.4802409341426818\n",
      "0.07075348616288982\n",
      "0.001550570359392322\n",
      "0.06913856937834448\n",
      "0.0019137169650011664\n",
      "0.04882220951258498\n",
      "0.028727861638374402\n",
      "0.014840775628162963\n",
      "0.00893908248383155\n",
      "0.996056782029964\n",
      "0.0039965304752982754\n",
      "0.31656805676103206\n",
      "0.04210209523335653\n",
      "0.008570333149604469\n",
      "0.1598526729703365\n",
      "0.7887857883214345\n",
      "2.0039766641353514\n",
      "0.01032299374471898\n",
      "0.16001689832974195\n",
      "0.16461539707957593\n",
      "0.0036190280702975983\n",
      "0.00016441841793338793\n",
      "0.0011143717265101722\n",
      "0.09157803024742153\n",
      "0.22031174529149627\n",
      "0.044920757377302584\n",
      "0.00042379941294622624\n",
      "0.021141643680486946\n",
      "0.013267717055590062\n",
      "0.08590606161236163\n",
      "0.0798918044418074\n",
      "0.03397512255445337\n",
      "0.004399747564085626\n",
      "0.0036023501022555125\n",
      "0.6480967915831337\n",
      "1.3313615365293074\n",
      "1.5283565188075248\n",
      "0.6881662751513751\n",
      "0.04112318336036308\n",
      "0.06381930159049765\n",
      "0.44907819150609657\n",
      "0.2839155329176384\n",
      "0.7614608463546835\n",
      "0.05786524398788832\n",
      "1.5105303887047146\n",
      "4.4681354408375915\n",
      "3.157125842053629\n",
      "1.1323453629976314\n",
      "0.6454299873582878\n",
      "0.008885977811301272\n",
      "0.035957832867660654\n",
      "0.25470543775651533\n",
      "0.0012281440010834253\n",
      "0.04799680446076784\n",
      "0.0003811193297887795\n",
      "0.00031011588841815336\n",
      "0.047953858878540215\n",
      "0.5978832019255778\n",
      "0.039625549953508386\n",
      "2.4414496831145316\n",
      "1.1958080900455488\n",
      "0.09004768810938388\n",
      "0.011109218869753298\n",
      "0.17573197539593746\n",
      "0.01616301993073622\n",
      "0.0033596767837297155\n",
      "0.014221804056128709\n",
      "0.3850432228214498\n",
      "0.020734769483828165\n",
      "5.368309745273213e-05\n",
      "0.8618462141037229\n",
      "0.215848845885333\n",
      "5.5805661891615124e-05\n",
      "0.3229348534614768\n",
      "0.004012707880352624\n",
      "0.7399107191810665\n",
      "1.5826221843018367\n",
      "0.20870066767742\n",
      "0.01937641488604327\n",
      "0.5793709238695957\n",
      "0.01593771978048836\n",
      "0.07260512386630537\n",
      "0.016911485408595293\n",
      "0.4362122731242364\n",
      "0.766675803147476\n",
      "0.0042086749624142775\n",
      "0.11181351245988662\n",
      "0.01089413846251018\n",
      "2.0946515469592994\n",
      "0.35899071760954465\n",
      "0.029684747218697377\n",
      "0.1953662510436977\n",
      "0.29245123100531745\n",
      "0.2304512672305914\n",
      "0.17143912184718257\n",
      "0.0952070626435617\n",
      "0.009696289391892387\n",
      "0.14553459648701583\n",
      "0.0005080200509870567\n",
      "0.6591519004695205\n",
      "1.0834166623465553\n",
      "0.141178408385271\n",
      "0.0814076552898357\n",
      "0.07840133330857782\n",
      "0.006433775818853696\n",
      "0.17071938960270872\n",
      "0.1881797076774971\n",
      "0.07970857209323703\n",
      "0.14671992612610404\n",
      "0.028441617156187964\n",
      "0.008575236994724126\n",
      "0.05090706841154395\n",
      "0.00933201181266551\n",
      "0.40571331006443867\n",
      "0.04252946074057431\n",
      "0.006756109638995281\n",
      "0.041278480927994375\n",
      "0.06377913819991188\n",
      "0.0013603082668238828\n",
      "0.15182093773402894\n",
      "0.002730296527126929\n",
      "0.005940712980891839\n",
      "7.37598529193801e-05\n",
      "0.0009263244504422793\n",
      "0.005373549925962475\n",
      "0.08962368448521943\n",
      "0.001065954718764673\n",
      "0.004206417333454173\n",
      "0.017211083689698044\n",
      "1.8633626970669197\n",
      "0.16817797462641523\n",
      "0.36204961505347727\n",
      "0.0007866957955986892\n",
      "0.004648636567334258\n",
      "0.024043793409529557\n",
      "0.20211554209584898\n",
      "0.0002684190817683864\n",
      "0.5251814821666809\n",
      "0.39523230567000883\n",
      "0.003064954479207017\n",
      "0.6041171661022071\n",
      "0.0009217483272397091\n",
      "0.0036321806795913\n",
      "0.20317766523832043\n",
      "0.0018000023753781656\n",
      "0.00697187028027145\n",
      "0.00013171668811862185\n",
      "0.9443827962338469\n",
      "3.219817737555896e-05\n",
      "0.09187461793285308\n",
      "0.00023086763940097986\n",
      "0.4254057593322478\n",
      "0.0766597603877507\n",
      "0.0003262266070891788\n",
      "0.0006944020937341389\n",
      "1.0209803303479823\n",
      "0.005027646530902019\n",
      "0.15179904972783861\n",
      "0.056055650430847134\n",
      "1.9131669056350677\n",
      "0.9019956833918604\n",
      "0.04189929304663142\n",
      "0.08832570644538675\n",
      "0.4524030342436018\n",
      "0.7086583722534201\n",
      "0.26639973307139314\n",
      "0.006162249555256185\n",
      "0.8126033775540991\n",
      "0.30221169316425966\n",
      "0.009778046866059644\n",
      "0.16364968462171017\n",
      "0.2817971713758934\n",
      "0.09810643801407289\n",
      "0.3287442668116235\n",
      "0.06742339144275367\n",
      "0.13363194620763236\n",
      "0.42540873940708535\n",
      "0.11311665718235957\n",
      "0.00018250844455513834\n",
      "0.8365871150723028\n",
      "0.1580316673531214\n",
      "1.6303220526593027\n",
      "0.06771052771255341\n",
      "0.013195754114981828\n",
      "0.10991412393380011\n",
      "0.0009708151222601685\n",
      "0.004493404216697963\n",
      "0.009725043613803544\n",
      "0.0002610279385962641\n",
      "9.40449019751571e-06\n",
      "0.00019511251209146642\n",
      "0.0039871985817237585\n",
      "0.012683923257092139\n",
      "0.0003106891291328582\n",
      "0.06536146301737347\n",
      "0.008203449435390395\n",
      "0.0006060784619836542\n",
      "0.0030269554734515987\n",
      "7.36650304986235e-05\n",
      "0.3351491380274404\n",
      "0.10279819317479126\n",
      "0.011122578000376193\n",
      "0.030343905575876677\n",
      "0.10850253913660775\n",
      "0.001394916555292607\n",
      "0.006945560826179172\n",
      "0.020620130581336445\n",
      "0.06104390120698283\n",
      "0.005264175513818423\n",
      "0.04309568199482046\n",
      "0.0008075333827426663\n",
      "0.06123389774713232\n",
      "0.002131502198021708\n",
      "0.08436030564713444\n",
      "0.12999468176223866\n",
      "0.0016049723235962913\n",
      "0.0015010715154993835\n",
      "0.012834661275249232\n",
      "0.014167665260423215\n",
      "0.04934348587057746\n",
      "0.0013842987456630918\n",
      "0.0019948354874548936\n",
      "0.3146346754716629\n",
      "0.14522696187440176\n",
      "0.04840468943968026\n",
      "0.032329515608393825\n",
      "0.0014804485893865976\n",
      "0.003179578706919521\n",
      "0.010306445238261094\n",
      "0.06771273392620498\n",
      "0.035449015474804155\n",
      "0.3835303530086974\n",
      "0.34205383905509634\n",
      "0.03796872321681094\n",
      "0.0008854827501457608\n",
      "0.0023851702911490344\n",
      "0.0014043196529314153\n",
      "0.17043995410443552\n",
      "0.0010139603811985683\n",
      "0.034849128848257506\n",
      "0.6797311982551711\n",
      "0.0012676704636298119\n",
      "0.003330381011806423\n",
      "4.493670816105751e-05\n",
      "0.018227380364410593\n",
      "0.018550569272252994\n",
      "7.299965787932783e-05\n",
      "0.6425408313240017\n",
      "0.9877144149440762\n",
      "5.1914759611484016e-06\n",
      "0.000299515241789366\n",
      "0.0004102315932052045\n",
      "0.0019247288251095344\n",
      "0.00022685055270188644\n",
      "0.00041518989008332703\n",
      "0.0015778587892840295\n",
      "0.0002582224560183303\n",
      "0.3170432804019388\n",
      "0.002233713791368978\n",
      "0.001106335327981033\n",
      "0.002185175181659789\n",
      "0.029106523051658412\n",
      "0.0006180589949040561\n",
      "0.14028123246716423\n",
      "0.007408142600668572\n",
      "0.0004021808810089615\n",
      "0.004733449285935146\n",
      "0.19607596751182624\n",
      "0.00013579294025248685\n",
      "1.175908931155246e-05\n",
      "0.010325992098948421\n",
      "0.019091692141229657\n",
      "0.0001588235937568906\n",
      "0.15499972052333313\n",
      "0.03279381964249846\n",
      "0.02163720646235927\n",
      "0.07652013256039761\n",
      "0.0038876580677178808\n",
      "8.40800392310584e-05\n",
      "0.23692703399551132\n",
      "0.0015677116906152723\n",
      "0.0983287474708987\n",
      "6.26316621368231e-05\n",
      "0.005404933607973906\n",
      "2.9675724951351347e-05\n",
      "3.743365350542532e-05\n",
      "0.0007832148178521798\n",
      "0.00958852432310001\n",
      "0.00044225247570093906\n",
      "9.391196736821882e-06\n",
      "1.793180240943165e-05\n",
      "0.02978668250604007\n",
      "0.00016984694003363833\n",
      "2.288748016487789e-05\n",
      "0.003037468677168641\n",
      "0.0023930228047414647\n",
      "0.0002777329386339826\n",
      "0.00017575070328990783\n",
      "4.1914235725100855e-06\n",
      "0.053908257496168475\n",
      "0.051186110810218365\n",
      "0.0007972156759321454\n",
      "0.005224604114657453\n",
      "0.00014806530416507693\n",
      "0.0003947204103592765\n",
      "0.0026798335277733475\n",
      "0.0023741998841285577\n",
      "0.0013225589480174314\n",
      "3.387617985140162e-05\n",
      "3.7949914895732225e-05\n",
      "0.000748379479800708\n",
      "0.008188464487972284\n",
      "1.7871437825976336e-05\n",
      "0.0016676356420838219\n",
      "0.006735507557214656\n",
      "0.0002224457404866336\n",
      "0.012889710313930366\n",
      "0.005524885870036736\n",
      "0.03928986511067532\n",
      "0.0013005743061978528\n",
      "2.127760074470512e-05\n",
      "0.00010331179820014623\n",
      "2.559021078406594e-07\n",
      "0.014024102194776877\n",
      "0.002210992223586886\n",
      "0.004089550589041345\n",
      "2.4650524855872437e-05\n",
      "0.0004068019958110956\n",
      "0.0016294808708677108\n",
      "0.0009501922348214625\n",
      "0.00046679678346262324\n",
      "accuracy: 0.9061\n"
     ]
    }
   ],
   "source": [
    "def modify_label(y):\n",
    "    out = np.zeros((10, y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        out[y[i,0], i] = 1\n",
    "    return out\n",
    "\n",
    "def encode_level(y_hat):\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        mx = -10\n",
    "        mx_idx = -1\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            if y_hat[i,j] > mx:\n",
    "                mx = y_hat[i,j]\n",
    "                mx_idx = i\n",
    "            y_hat[i][j] = 0\n",
    "        y_hat[mx_idx, j] = 1\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def calc_accuracy(y_hat, y):\n",
    "    match = 0\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        flag = 0\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            if y_hat[i,j] != y[i,j]:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            match += 1\n",
    "\n",
    "    return match\n",
    "\n",
    "\n",
    "def run_mnist():\n",
    "    f1 = open(\"architecture.txt\", \"r\")\n",
    "    lines = f1.readlines()\n",
    "    cnn_layers = list()\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        if words[0].lower() == \"fc\":\n",
    "            cnn_layers.append(FullyConnected(int(words[1])))\n",
    "        elif words[0].lower() == \"relu\":\n",
    "            cnn_layers.append(ReLU())\n",
    "        elif words[0].lower() == \"softmax\":\n",
    "            cnn_layers.append(SoftMax())\n",
    "        elif words[0].lower() == \"conv\":\n",
    "            cnn_layers.append(Convolutional( int(words[1]), int(words[2]), int(words[3]), int(words[4])))\n",
    "        elif words[0].lower() == \"pool\":\n",
    "            cnn_layers.append(MaxPool(int(words[1]), int(words[2])))\n",
    "        elif words[0].lower() == \"flatten\":\n",
    "            cnn_layers.append(Flattening())\n",
    "\n",
    "    f1.close()\n",
    "\n",
    "    x_mnist_train, y_mnist_train = read_mnist_train()\n",
    "    batch_sz = 5\n",
    "\n",
    "    itr_outer = 10\n",
    "\n",
    "    while itr_outer > 0:\n",
    "        itr_inner = 100\n",
    "        for i in range(0,x_mnist_train.shape[0],batch_sz):\n",
    "            curr_batch_x = x_mnist_train[i:i+batch_sz,:]\n",
    "            curr_batch_y = y_mnist_train[i:i+batch_sz]\n",
    "\n",
    "            curr_batch_x = curr_batch_x.reshape((batch_sz, 28, 28, 1))\n",
    "            curr_batch_y = curr_batch_y.reshape(batch_sz, 1)\n",
    "\n",
    "            curr_batch_y = modify_label(curr_batch_y)\n",
    "\n",
    "            prev_a = curr_batch_x\n",
    "            for layer in cnn_layers:\n",
    "                prev_a = layer.forward(prev_a)\n",
    "\n",
    "            prev_derivative = curr_batch_y\n",
    "            for j in range(len(cnn_layers)-1,0,-1):\n",
    "                prev_derivative = cnn_layers[j].backward(prev_derivative)\n",
    "\n",
    "            print(cross_entropy(prev_a, curr_batch_y))\n",
    "\n",
    "            itr_inner -= 1\n",
    "            if itr_inner <= 0:\n",
    "                break\n",
    "\n",
    "        itr_outer -= 1\n",
    "\n",
    "\n",
    "    x_mnist_test, y_mnist_test = read_mnist_test()\n",
    "\n",
    "    total_match = 0\n",
    "\n",
    "    for i in range(0,x_mnist_test.shape[0],batch_sz):\n",
    "        curr_batch_x = x_mnist_test[i:i+batch_sz,:]\n",
    "        curr_batch_y = y_mnist_test[i:i+batch_sz]\n",
    "\n",
    "        curr_batch_x = curr_batch_x.reshape((batch_sz, 28, 28, 1))\n",
    "        curr_batch_y = curr_batch_y.reshape(batch_sz, 1)\n",
    "\n",
    "        curr_batch_y = modify_label(curr_batch_y)\n",
    "\n",
    "        prev_a = curr_batch_x\n",
    "        for layer in cnn_layers:\n",
    "            prev_a = layer.forward(prev_a)\n",
    "\n",
    "        prev_a = encode_level(prev_a)\n",
    "        calc_accuracy(prev_a, curr_batch_y)\n",
    "\n",
    "        total_match += calc_accuracy(prev_a, curr_batch_y)\n",
    "\n",
    "\n",
    "    print(\"accuracy: \" + str(total_match/10000))\n",
    "\n",
    "\n",
    "\n",
    "run_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def read_data(file_path):\n",
    "#     df = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "#     num_features = df.shape[1] - 1\n",
    "#     df = pd.get_dummies(df, columns=[4], drop_first=False)\n",
    "#     train_dataset = df.to_numpy()\n",
    "#     x_train = train_dataset[:,:num_features]\n",
    "#     y_train = train_dataset[:,num_features:]\n",
    "#\n",
    "#     x_train = x_train.T\n",
    "#     y_train = y_train.T\n",
    "#\n",
    "#     # print(x_train.shape)\n",
    "#     # print(y_train.shape)\n",
    "#     return x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def encode_level(y_hat):\n",
    "#     for j in range(y_hat.shape[1]):\n",
    "#         mx = -10\n",
    "#         mx_idx = -1\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             if y_hat[i,j] > mx:\n",
    "#                 mx = y_hat[i,j]\n",
    "#                 mx_idx = i\n",
    "#             y_hat[i][j] = 0\n",
    "#         y_hat[mx_idx, j] = 1\n",
    "#\n",
    "#     return y_hat\n",
    "#\n",
    "# def calc_accuracy(y_hat, y):\n",
    "#     match = 0\n",
    "#     for j in range(y_hat.shape[1]):\n",
    "#         flag = 0\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             if y_hat[i,j] != y[i,j]:\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 0:\n",
    "#             match += 1\n",
    "#\n",
    "#     print(\"accuracy: \" + str(match/y_hat.shape[1]))\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# def runcnn():\n",
    "#     f1 = open(\"architecture.txt\", \"r\")\n",
    "#     lines = f1.readlines()\n",
    "#     cnn_layers = list()\n",
    "#     for line in lines:\n",
    "#         words = line.strip().split()\n",
    "#         if words[0].lower() == \"fc\":\n",
    "#             cnn_layers.append(FullyConnected(int(words[1])))\n",
    "#         elif words[0].lower() == \"relu\":\n",
    "#             cnn_layers.append(ReLU())\n",
    "#         elif words[0].lower() == \"softmax\":\n",
    "#             cnn_layers.append(SoftMax())\n",
    "#\n",
    "#     f1.close()\n",
    "#\n",
    "#     x, y = read_data(\"Toy Dataset/trainNN.txt\")\n",
    "#     itr_limit = 10000\n",
    "#     for itr in range(itr_limit):\n",
    "#         prev_a = x\n",
    "#         for layer in cnn_layers:\n",
    "#             prev_a = layer.forward(prev_a)\n",
    "#\n",
    "#         prev_derivative = y\n",
    "#         for i in range(len(cnn_layers)-1,0,-1):\n",
    "#             prev_derivative = cnn_layers[i].backward(prev_derivative)\n",
    "#\n",
    "#         if itr % 500 == 0:\n",
    "#             print(cross_entropy(prev_a, y))\n",
    "#\n",
    "#     prev_a = encode_level(prev_a)\n",
    "#     calc_accuracy(prev_a, y)\n",
    "#\n",
    "#     x_test, y_test = read_data(\"Toy Dataset/testNN.txt\")\n",
    "#\n",
    "#     prev_a = x_test\n",
    "#     for itr in range(itr_limit):\n",
    "#         prev_a = x\n",
    "#         for layer in cnn_layers:\n",
    "#             prev_a = layer.forward(prev_a)\n",
    "#\n",
    "#     prev_a = encode_level(prev_a)\n",
    "#     calc_accuracy(prev_a, y_test)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# run cnn\n",
    "# \"\"\"\n",
    "# runcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}