{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "alpha = 0.01\n",
    "itr_limit = 2000\n",
    "\n",
    "dbg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z = wx + b\n",
    "\"\"\"\n",
    "class FullyConnected:\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.a = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.z = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, a):\n",
    "        if self.w is None or self.b is None:\n",
    "            self.w = np.random.random((self.out_dim, a.shape[0]))*0.01\n",
    "            self.b = np.zeros((self.out_dim, 1))\n",
    "\n",
    "        self.a = a\n",
    "        self.z = np.dot(self.w, a) + self.b\n",
    "\n",
    "        if dbg:\n",
    "            print(\"fc_forward: \")\n",
    "            print(self.z.shape)\n",
    "\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m = dz.shape[1]\n",
    "        self.dw = np.matmul(dz, self.a.T)/m\n",
    "        self.db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "\n",
    "        self.w = self.w - alpha*self.dw\n",
    "        self.b = self.b - alpha*self.db\n",
    "\n",
    "        da = np.matmul(self.w.T, dz)\n",
    "        return da\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = ReLU(z)\n",
    "\"\"\"\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.out_dim = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        r = np.maximum(0, z)\n",
    "        return r\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        dz = np.array(z, copy=True)\n",
    "        dz[dz<=0] = 0\n",
    "        dz[dz>0] = 1\n",
    "        return dz\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.out_dim is None:\n",
    "            self.out_dim = z.shape[0]\n",
    "\n",
    "        self.z = z\n",
    "        self.a = self.relu(z)\n",
    "\n",
    "        if dbg:\n",
    "            print(\"relu_forward: \")\n",
    "            print(self.a.shape)\n",
    "\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da):\n",
    "        dz = np.multiply(da, self.relu_derivative(self.z))\n",
    "        return dz\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    m = y.shape[1]\n",
    "    logs = np.multiply(np.log(y_hat),y)\n",
    "    cost = - np.sum(logs) / m\n",
    "    return cost\n",
    "\n",
    "\"\"\"\n",
    "y_hat = e^z/sum(e^x)\n",
    "\"\"\"\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.out_dim = None\n",
    "        self.z = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.out_dim is None:\n",
    "            self.out_dim = z.shape[0]\n",
    "\n",
    "        self.z = z\n",
    "        self.y_hat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "        if dbg:\n",
    "            print(\"soft_forward: \")\n",
    "            print(self.y_hat.shape)\n",
    "\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y):\n",
    "        dz = self.y_hat - y\n",
    "        return dz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    df = pd.read_csv(\"Toy Dataset/trainNN.txt\", delim_whitespace=True, header=None)\n",
    "    num_features = df.shape[1] - 1\n",
    "    df = pd.get_dummies(df, columns=[4], drop_first=False)\n",
    "    train_dataset = df.to_numpy()\n",
    "    x_train = train_dataset[:,:num_features]\n",
    "    y_train = train_dataset[:,num_features:]\n",
    "\n",
    "    x_train = x_train.T\n",
    "    y_train = y_train.T\n",
    "\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    return x_train, y_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def run_cnn():\n",
    "    f1 = open(\"architecture.txt\", \"r\")\n",
    "    lines = f1.readlines()\n",
    "    cnn_layers = list()\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        if words[0].lower() == \"fc\":\n",
    "            cnn_layers.append(FullyConnected(int(words[1])))\n",
    "        elif words[0].lower() == \"relu\":\n",
    "            cnn_layers.append(ReLU())\n",
    "        elif words[0].lower() == \"softmax\":\n",
    "            cnn_layers.append(SoftMax())\n",
    "\n",
    "    f1.close()\n",
    "\n",
    "    x, y = read_data()\n",
    "\n",
    "    for itr in range(itr_limit):\n",
    "        prev_a = x\n",
    "        for layer in cnn_layers:\n",
    "            prev_a = layer.forward(prev_a)\n",
    "\n",
    "        prev_derivative = y\n",
    "        for i in range(len(cnn_layers)-1,0,-1):\n",
    "            prev_derivative = cnn_layers[i].backward(prev_derivative)\n",
    "\n",
    "        if itr % 500 == 0:\n",
    "            print(cross_entropy(prev_a, y))\n",
    "\n",
    "\n",
    "    print(prev_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3879759199347623\n",
      "1.1636866632742078\n",
      "1.0622854708411886\n",
      "0.9857946152906494\n",
      "[[0.4394747  0.19842716 0.01587462 ... 0.44747162 0.20219324 0.01506944]\n",
      " [0.292706   0.28767216 0.11156708 ... 0.29121304 0.28876816 0.10892933]\n",
      " [0.17714405 0.29177945 0.32273361 ... 0.17353359 0.28997862 0.32108436]\n",
      " [0.09067525 0.22212123 0.54982469 ... 0.08778175 0.21905998 0.55491686]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run cnn\n",
    "\"\"\"\n",
    "run_cnn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}