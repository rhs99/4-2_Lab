{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "dbg = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "def read_mnist():\n",
    "    x, y = loadlocal_mnist(images_path='MNIST/train-images.idx3-ubyte',\n",
    "            labels_path='MNIST/train-labels.idx1-ubyte')\n",
    "    return x, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z = wx + b\n",
    "\"\"\"\n",
    "class FullyConnected:\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.a = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.z = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, a):\n",
    "        if self.w is None or self.b is None:\n",
    "            self.w = np.random.random((self.out_dim, a.shape[0]))*0.001\n",
    "            self.b = np.zeros((self.out_dim, 1))\n",
    "\n",
    "        self.a = a\n",
    "        self.z = np.dot(self.w, a) + self.b\n",
    "\n",
    "        if dbg:\n",
    "            print(\"fc_forward: \")\n",
    "            print(self.z.shape)\n",
    "\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m = dz.shape[1]\n",
    "        self.dw = np.matmul(dz, self.a.T)/m\n",
    "        self.db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "        self.w = self.w - alpha*self.dw\n",
    "        self.b = self.b - alpha*self.db\n",
    "        da = np.matmul(self.w.T, dz)\n",
    "        return da\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = ReLU(z)\n",
    "\"\"\"\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        r = np.maximum(0, z)\n",
    "        return r\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        dz = np.array(z, copy=True)\n",
    "        dz[dz<=0] = 0\n",
    "        dz[dz>0] = 1\n",
    "        return dz\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        self.a = self.relu(z)\n",
    "        if dbg:\n",
    "            print(\"relu_forward: \")\n",
    "            print(self.a.shape)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da):\n",
    "        dz = np.multiply(da, self.relu_derivative(self.z))\n",
    "        return dz\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    m = y.shape[1]\n",
    "    logs = np.multiply(np.log(y_hat),y)\n",
    "    cost = - np.sum(logs) / m\n",
    "    return cost\n",
    "\n",
    "\"\"\"\n",
    "y_hat = e^z/sum(e^x)\n",
    "\"\"\"\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.out_dim = None\n",
    "        self.z = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.out_dim is None:\n",
    "            self.out_dim = z.shape[0]\n",
    "        self.z = z\n",
    "        self.y_hat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        if dbg:\n",
    "            print(\"soft_forward: \")\n",
    "            print(self.y_hat.shape)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y):\n",
    "        dz = self.y_hat - y\n",
    "        return dz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "class Convolutional:\n",
    "    def __init__(self, number_of_filters, filter_dim, stride=1, padding = 0):\n",
    "        self.number_of_filters = number_of_filters\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.a_prev = None\n",
    "\n",
    "    def zero_pad(self, x, pad):\n",
    "        x_pad = np.pad(x, ((0,0), (pad, pad), (pad, pad), (0,0)), mode='constant', constant_values = (0,0))\n",
    "        return x_pad\n",
    "\n",
    "    def conv_single_step(self, a_slice_prev, w, b):\n",
    "        s = np.multiply(a_slice_prev, w)\n",
    "        z = np.sum(s)\n",
    "        z = z + float(b)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = a_prev.shape[0], a_prev.shape[1], a_prev.shape[2], a_prev.shape[3]\n",
    "        if self.w is None:\n",
    "            self.w = np.random.random((self.filter_dim, self.filter_dim, n_C_prev, self.number_of_filters))*0.001\n",
    "            self.b = np.zeros((1, 1, 1, self.number_of_filters))\n",
    "\n",
    "        (f_H, f_W, n_C_prev, n_C) = self.w.shape[0], self.w.shape[1], self.w.shape[2], self.w.shape[3]\n",
    "\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "\n",
    "        n_H = int(int(n_H_prev + 2*pad - f_H)/stride + 1)\n",
    "        n_W = int(int(n_W_prev + 2*pad - f_W)/stride + 1)\n",
    "\n",
    "        z = np.zeros([m, n_H, n_W, n_C])\n",
    "        a_prev_pad = self.zero_pad(a_prev, pad)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_pad_cur = a_prev_pad[i]\n",
    "            for h in range(n_H):\n",
    "                vert_start = stride * h\n",
    "                vert_end = vert_start + f_H\n",
    "                for w in range(n_W):\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = horiz_start + f_W\n",
    "                    for c in range(n_C):\n",
    "                        a_slice_prev = a_prev_pad_cur[ vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        weights = self.w[:, :, :, c]\n",
    "                        biases = self.b[:, :, :, c]\n",
    "                        z[i, h, w, c] = self.conv_single_step(a_slice_prev, weights, biases)\n",
    "\n",
    "        assert(z.shape == (m, n_H, n_W, n_C))\n",
    "        if dbg:\n",
    "            print(\"conv_forward: \")\n",
    "            print(z.shape)\n",
    "        return z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = self.a_prev.shape\n",
    "        (f, f, n_C_prev, n_C) = self.w.shape\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        (m, n_H, n_W, n_C) = dz.shape\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "        dw = np.zeros(self.w.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "        a_prev_pad = self.zero_pad(self.a_prev, pad)\n",
    "        da_prev_pad = self.zero_pad(da_prev, pad)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_pad_cur = a_prev_pad[i]\n",
    "            da_prev_pad_cur = da_prev_pad[i]\n",
    "\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C):\n",
    "                        vert_start = stride * h\n",
    "                        vert_end = vert_start + f\n",
    "                        horiz_start = stride * w\n",
    "                        horiz_end = horiz_start + f\n",
    "\n",
    "                        a_slice = a_prev_pad_cur[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                        da_prev_pad_cur[vert_start:vert_end, horiz_start:horiz_end, :] += self.w[:,:,:,c] * dz[i, h, w, c]\n",
    "                        dw[:,:,:,c] += a_slice * dz[i, h, w, c]\n",
    "                        db[:,:,:,c] += dz[i, h, w, c]\n",
    "\n",
    "            if pad > 0:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[pad:-pad, pad:-pad, :]\n",
    "            else:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[:, :, :]\n",
    "\n",
    "\n",
    "\n",
    "        assert(da_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "        self.w = self.w - alpha*dw\n",
    "        self.b = self.b - alpha*db\n",
    "\n",
    "        return da_prev\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = a_prev.shape\n",
    "\n",
    "        f = self.filter_dim\n",
    "        stride = self.stride\n",
    "\n",
    "        n_H = int(1 + (n_H_prev - f) / stride)\n",
    "        n_W = int(1 + (n_W_prev - f) / stride)\n",
    "        n_C = n_C_prev\n",
    "\n",
    "        a = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "        for i in range(m):\n",
    "            for h in range(n_H):\n",
    "                vert_start = stride * h\n",
    "                vert_end = vert_start + f\n",
    "                for w in range(n_W):\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    for c in range (n_C):\n",
    "                        a_prev_slice = a_prev[i]\n",
    "                        a[i, h, w, c] = np.max(a_prev_slice[vert_start:vert_end, horiz_start:horiz_end, c])\n",
    "\n",
    "        assert(a.shape == (m, n_H, n_W, n_C))\n",
    "        if dbg:\n",
    "            print(\"pool_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "\n",
    "    def create_mask_from_window(self, x):\n",
    "        mask = (x == np.max(x))\n",
    "        return mask\n",
    "\n",
    "    def backward(self, da):\n",
    "        stride = self.stride\n",
    "        f = self.filter_dim\n",
    "\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = self.a_prev.shape\n",
    "        m, n_H, n_W, n_C = da.shape\n",
    "\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev = self.a_prev[i,:,:,:]\n",
    "\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C):\n",
    "                        vert_start  = h * stride\n",
    "                        vert_end    = h * stride + f\n",
    "                        horiz_start = w * stride\n",
    "                        horiz_end   = w * stride + f\n",
    "\n",
    "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
    "                        mask = self.create_mask_from_window( a_prev_slice )\n",
    "                        da_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * da[i, h, w, c]\n",
    "\n",
    "        assert(da_prev.shape == self.a_prev.shape)\n",
    "        return da_prev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "class Flattening:\n",
    "    def __init__(self):\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        m = a_prev.shape[0]\n",
    "        a = list()\n",
    "        for i in range(m):\n",
    "            a.append(np.ravel(a_prev[i,:,:,:]))\n",
    "        a = np.array(a)\n",
    "        a = a.T\n",
    "        if dbg:\n",
    "            print(\"flatten_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        da_prev = da.reshape(self.a_prev.shape)\n",
    "        return da_prev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025873110561657\n",
      "2.3025800238825314\n",
      "2.30224424416095\n",
      "2.3030525896673493\n",
      "2.3020605230545335\n",
      "2.302508413560533\n",
      "2.3030388871191314\n",
      "2.3014207442251395\n",
      "2.3023827387116778\n",
      "2.301717831398491\n"
     ]
    }
   ],
   "source": [
    "def modify_label(y):\n",
    "    out = np.zeros((10, y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        out[y[i,0], i] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_mnist():\n",
    "    f1 = open(\"architecture.txt\", \"r\")\n",
    "    lines = f1.readlines()\n",
    "    cnn_layers = list()\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        if words[0].lower() == \"fc\":\n",
    "            cnn_layers.append(FullyConnected(int(words[1])))\n",
    "        elif words[0].lower() == \"relu\":\n",
    "            cnn_layers.append(ReLU())\n",
    "        elif words[0].lower() == \"softmax\":\n",
    "            cnn_layers.append(SoftMax())\n",
    "        elif words[0].lower() == \"conv\":\n",
    "            cnn_layers.append(Convolutional( int(words[1]), int(words[2]), int(words[3]), int(words[4])))\n",
    "        elif words[0].lower() == \"pool\":\n",
    "            cnn_layers.append(MaxPool(int(words[1]), int(words[2])))\n",
    "        elif words[0].lower() == \"flatten\":\n",
    "            cnn_layers.append(Flattening())\n",
    "\n",
    "    f1.close()\n",
    "\n",
    "    itr = 10\n",
    "\n",
    "    x_mnist_train, y_mnist_train = read_mnist()\n",
    "    batch_sz = 50\n",
    "    for i in range(0,x_mnist_train.shape[0],batch_sz):\n",
    "        curr_batch_x = x_mnist_train[i:i+batch_sz,:]\n",
    "        curr_batch_y = y_mnist_train[i:i+batch_sz]\n",
    "\n",
    "        curr_batch_x = curr_batch_x.reshape((batch_sz, 28, 28, 1))\n",
    "        curr_batch_y = curr_batch_y.reshape(batch_sz, 1)\n",
    "\n",
    "        curr_batch_y = modify_label(curr_batch_y)\n",
    "\n",
    "        prev_a = curr_batch_x\n",
    "        for layer in cnn_layers:\n",
    "            prev_a = layer.forward(prev_a)\n",
    "\n",
    "        prev_derivative = curr_batch_y\n",
    "        for j in range(len(cnn_layers)-1,0,-1):\n",
    "            prev_derivative = cnn_layers[j].backward(prev_derivative)\n",
    "\n",
    "        print(cross_entropy(prev_a, curr_batch_y))\n",
    "\n",
    "        itr -= 1\n",
    "        if itr <= 0:\n",
    "            break\n",
    "\n",
    "\n",
    "run_mnist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "# def read_data(file_path):\n",
    "#     df = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "#     num_features = df.shape[1] - 1\n",
    "#     df = pd.get_dummies(df, columns=[4], drop_first=False)\n",
    "#     train_dataset = df.to_numpy()\n",
    "#     x_train = train_dataset[:,:num_features]\n",
    "#     y_train = train_dataset[:,num_features:]\n",
    "#\n",
    "#     x_train = x_train.T\n",
    "#     y_train = y_train.T\n",
    "#\n",
    "#     # print(x_train.shape)\n",
    "#     # print(y_train.shape)\n",
    "#     return x_train, y_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "# def modify_level(y_hat):\n",
    "#     for j in range(y_hat.shape[1]):\n",
    "#         mx = -10\n",
    "#         mx_idx = -1\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             if y_hat[i,j] > mx:\n",
    "#                 mx = y_hat[i,j]\n",
    "#                 mx_idx = i\n",
    "#             y_hat[i][j] = 0\n",
    "#         y_hat[mx_idx, j] = 1\n",
    "#\n",
    "#     return y_hat\n",
    "#\n",
    "# def calc_accuracy(y_hat, y):\n",
    "#     match = 0\n",
    "#     for j in range(y_hat.shape[1]):\n",
    "#         flag = 0\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             if y_hat[i,j] != y[i,j]:\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 0:\n",
    "#             match += 1\n",
    "#\n",
    "#     print(\"accuracy: \" + str(match/y_hat.shape[1]))\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# def run_cnn():\n",
    "#     f1 = open(\"architecture.txt\", \"r\")\n",
    "#     lines = f1.readlines()\n",
    "#     cnn_layers = list()\n",
    "#     for line in lines:\n",
    "#         words = line.strip().split()\n",
    "#         if words[0].lower() == \"fc\":\n",
    "#             cnn_layers.append(FullyConnected(int(words[1])))\n",
    "#         elif words[0].lower() == \"relu\":\n",
    "#             cnn_layers.append(ReLU())\n",
    "#         elif words[0].lower() == \"softmax\":\n",
    "#             cnn_layers.append(SoftMax())\n",
    "#\n",
    "#     f1.close()\n",
    "#\n",
    "#     x, y = read_data(\"Toy Dataset/trainNN.txt\")\n",
    "#\n",
    "#     for itr in range(itr_limit):\n",
    "#         prev_a = x\n",
    "#         for layer in cnn_layers:\n",
    "#             prev_a = layer.forward(prev_a)\n",
    "#\n",
    "#         prev_derivative = y\n",
    "#         for i in range(len(cnn_layers)-1,0,-1):\n",
    "#             prev_derivative = cnn_layers[i].backward(prev_derivative)\n",
    "#\n",
    "#         # if itr % 500 == 0:\n",
    "#         #     print(cross_entropy(prev_a, y))\n",
    "#\n",
    "#     # prev_a = modify_level(prev_a)\n",
    "#     # calc_accuracy(prev_a, y)\n",
    "#\n",
    "#     x_test, y_test = read_data(\"Toy Dataset/testNN.txt\")\n",
    "#\n",
    "#     prev_a = x_test\n",
    "#     for itr in range(itr_limit):\n",
    "#         prev_a = x\n",
    "#         for layer in cnn_layers:\n",
    "#             prev_a = layer.forward(prev_a)\n",
    "#\n",
    "#     prev_a = modify_level(prev_a)\n",
    "#     calc_accuracy(prev_a, y_test)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# run cnn\n",
    "# \"\"\"\n",
    "# # run_cnn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}