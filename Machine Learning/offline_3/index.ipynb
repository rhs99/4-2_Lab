{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "alpha = 0.01\n",
    "INIT_W = 0.01\n",
    "\n",
    "\n",
    "dbg = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_mnist_train():\n",
    "    x, y = loadlocal_mnist(images_path='MNIST/train-images.idx3-ubyte',\n",
    "            labels_path='MNIST/train-labels.idx1-ubyte')\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_mnist_test():\n",
    "    x, y = loadlocal_mnist(images_path='MNIST/t10k-images.idx3-ubyte',\n",
    "            labels_path='MNIST/t10k-labels.idx1-ubyte')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z = wx + b\n",
    "\"\"\"\n",
    "class FullyConnected:\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.a = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.z = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, a):\n",
    "        if self.w is None or self.b is None:\n",
    "            self.w = np.random.randn(self.out_dim, a.shape[0])*INIT_W\n",
    "            self.b = np.zeros((self.out_dim, 1))\n",
    "\n",
    "        self.a = a\n",
    "        self.z = np.matmul(self.w, a) + self.b\n",
    "\n",
    "        if dbg:\n",
    "            print(\"fc_forward: \")\n",
    "            print(self.z.shape)\n",
    "\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m = dz.shape[1]\n",
    "        self.dw = np.matmul(dz, self.a.T)/m\n",
    "        self.db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "        da = np.matmul(self.w.T, dz)\n",
    "        self.w = self.w - alpha*self.dw\n",
    "        self.b = self.b - alpha*self.db\n",
    "        return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = ReLU(z)\n",
    "\"\"\"\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        r = np.maximum(0, z)\n",
    "        return r\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        dz = np.array(z, copy=True)\n",
    "        dz[dz<=0] = 0\n",
    "        dz[dz>0] = 1\n",
    "        return dz\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        self.a = self.relu(z)\n",
    "        if dbg:\n",
    "            print(\"relu_forward: \")\n",
    "            print(self.a.shape)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da):\n",
    "        dz = np.multiply(da, self.relu_derivative(self.z))\n",
    "        return dz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    m = y.shape[1]\n",
    "    logs = np.multiply(np.log(y_hat),y)\n",
    "    cost = - np.sum(logs) / m\n",
    "    return cost\n",
    "\n",
    "\"\"\"\n",
    "y_hat = e^z/sum(e^x)\n",
    "\"\"\"\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.out_dim = None\n",
    "        self.z = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.out_dim is None:\n",
    "            self.out_dim = z.shape[0]\n",
    "        self.z = z\n",
    "        self.y_hat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        if dbg:\n",
    "            print(\"soft_forward: \")\n",
    "            print(self.y_hat.shape)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y):\n",
    "        dz = self.y_hat - y\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Convolutional:\n",
    "    def __init__(self, number_of_filters, filter_dim, stride=1, padding = 0):\n",
    "        self.number_of_filters = number_of_filters\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.a_prev = None\n",
    "\n",
    "    @staticmethod\n",
    "    def add_padd(x, pad):\n",
    "        x_pad = np.pad(x, ((0,0), (pad, pad), (pad, pad), (0,0)), mode='constant', constant_values = (0,0))\n",
    "        return x_pad\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_single_step(a_slice_prev, w, b):\n",
    "        s = np.multiply(a_slice_prev, w)\n",
    "        z = np.sum(s)\n",
    "        z = z + float(b)\n",
    "        return z\n",
    "\n",
    "    def convolve(self, x, y, b, pad,m, stride, nh, nw, nc, f):\n",
    "        x_p = self.add_padd(x, pad)\n",
    "        z = np.zeros([m, nh, nw, nc])\n",
    "        for i in range(m):\n",
    "            x_cur = x_p[i]\n",
    "            for h in range(nh):\n",
    "                v_shuru = stride * h\n",
    "                v_shesh = v_shuru + f\n",
    "                for w in range(nw):\n",
    "                    h_shuru = stride * w\n",
    "                    h_shesh = h_shuru + f\n",
    "                    for c in range(nc):\n",
    "                        x_slice = x_cur[ v_shuru:v_shesh, h_shuru:h_shesh, :]\n",
    "                        weights = y[:, :, :, c]\n",
    "                        biases = b[:, :, :, c]\n",
    "                        z[i, h, w, c] = self.conv_single_step(x_slice, weights, biases)\n",
    "        return z\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, nh_prev, nw_prev, nc_prev) = a_prev.shape\n",
    "\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(self.filter_dim, self.filter_dim, nc_prev, self.number_of_filters)*INIT_W\n",
    "            self.b = np.zeros((1, 1, 1, self.number_of_filters))\n",
    "\n",
    "        (fh, fw, nc_prev, nc) = self.w.shape\n",
    "\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "\n",
    "        nh = int(int(nh_prev + 2*pad - fh)/stride + 1)\n",
    "        nw = int(int(nw_prev + 2*pad - fw)/stride + 1)\n",
    "\n",
    "        z = self.convolve(a_prev,self.w,self.b,pad,m,self.stride,nh,nw,nc,fh)\n",
    "\n",
    "        assert(z.shape == (m, nh, nw, nc))\n",
    "        if dbg:\n",
    "            print(\"conv_forward: \")\n",
    "            print(z.shape)\n",
    "        return z\n",
    "\n",
    "    def backward(self, dz):\n",
    "        (m, nh_prev, nw_prev, nc_prev) = self.a_prev.shape\n",
    "        (fh, fw, nc_prev, nc) = self.w.shape\n",
    "        stride = self.stride\n",
    "        pad = self.padding\n",
    "        (m, nh, nw, nc) = dz.shape\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "        dw = np.zeros(self.w.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "        a_prev_pad = self.add_padd(self.a_prev, pad)\n",
    "        da_prev_pad = self.add_padd(da_prev, pad)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_pad_cur = a_prev_pad[i]\n",
    "            da_prev_pad_cur = da_prev_pad[i]\n",
    "            for h in range(nh):\n",
    "                for w in range(nw):\n",
    "                    for c in range(nc):\n",
    "                        v_shuru = stride * h\n",
    "                        v_shesh = v_shuru + fh\n",
    "                        h_shuru = stride * w\n",
    "                        h_shesh = h_shuru + fw\n",
    "\n",
    "                        a_slice = a_prev_pad_cur[v_shuru:v_shesh,h_shuru:h_shesh,:]\n",
    "                        da_prev_pad_cur[v_shuru:v_shesh, h_shuru:h_shesh, :] += self.w[:,:,:,c] * dz[i, h, w, c]\n",
    "                        dw[:,:,:,c] += a_slice * dz[i, h, w, c]\n",
    "                        db[:,:,:,c] += dz[i, h, w, c]\n",
    "\n",
    "            if pad > 0:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[pad:-pad, pad:-pad, :]\n",
    "            else:\n",
    "                da_prev[i, :, :, :] = da_prev_pad_cur[:, :, :]\n",
    "\n",
    "        assert(da_prev.shape == (m, nh_prev, nw_prev, nc_prev))\n",
    "\n",
    "        self.w = self.w - alpha*dw\n",
    "        self.b = self.b - alpha*db\n",
    "\n",
    "        return da_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        (m, nh_prev, nw_prev, nc_prev) = a_prev.shape\n",
    "        f = self.filter_dim\n",
    "        stride = self.stride\n",
    "        nh = int(1 + (nh_prev - f) / stride)\n",
    "        nw = int(1 + (nw_prev - f) / stride)\n",
    "        nc = nc_prev\n",
    "\n",
    "        a = np.zeros((m, nh, nw, nc))\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev_cur = a_prev[i]\n",
    "            for h in range(nh):\n",
    "                v_shuru = stride * h\n",
    "                v_shesh = v_shuru + f\n",
    "                for w in range(nw):\n",
    "                    h_shuru = stride * w\n",
    "                    h_shesh = h_shuru + f\n",
    "                    for c in range (nc):\n",
    "                        a_prev_cur_slice = a_prev_cur[v_shuru:v_shesh,h_shuru:h_shesh,c]\n",
    "                        a[i, h, w, c] = np.max(a_prev_cur_slice)\n",
    "\n",
    "        assert(a.shape == (m, nh, nw, nc))\n",
    "        if dbg:\n",
    "            print(\"pool_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mask_from_window(x):\n",
    "        mask = (x == np.max(x))\n",
    "        return mask\n",
    "\n",
    "    def backward(self, da):\n",
    "        stride = self.stride\n",
    "        f = self.filter_dim\n",
    "        (m, nh, nw, nc) = da.shape\n",
    "\n",
    "        da_prev = np.zeros(self.a_prev.shape)\n",
    "\n",
    "        for i in range(m):\n",
    "            a_prev = self.a_prev[i,:,:,:]\n",
    "\n",
    "            for h in range(nh):\n",
    "                for w in range(nw):\n",
    "                    for c in range(nc):\n",
    "                        v_shuru  = h * stride\n",
    "                        v_shesh    = v_shuru + f\n",
    "                        h_shuru = w * stride\n",
    "                        h_shesh   = h_shuru + f\n",
    "\n",
    "                        a_prev_slice = a_prev[ v_shuru:v_shesh, h_shuru:h_shesh, c ]\n",
    "                        mask = self.create_mask_from_window( a_prev_slice )\n",
    "                        da_prev[i, v_shuru:v_shesh, h_shuru:h_shesh, c] += mask * da[i, h, w, c]\n",
    "\n",
    "        assert(da_prev.shape == self.a_prev.shape)\n",
    "        return da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flattening:\n",
    "    def __init__(self):\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        m = a_prev.shape[0]\n",
    "        a = list()\n",
    "        for i in range(m):\n",
    "            a.append(np.ravel(a_prev[i,:,:,:]))\n",
    "        a = np.array(a)\n",
    "        a = a.T\n",
    "        if dbg:\n",
    "            print(\"flatten_forward: \")\n",
    "            print(a.shape)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        da = da.T\n",
    "        da_prev = da.reshape(self.a_prev.shape)\n",
    "        return da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def encode_level(y_hat):\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        mx = -10\n",
    "        mx_idx = -1\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            if y_hat[i,j] > mx:\n",
    "                mx = y_hat[i,j]\n",
    "                mx_idx = i\n",
    "            y_hat[i][j] = 0\n",
    "        y_hat[mx_idx, j] = 1\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def calc_accuracy(y_hat, y):\n",
    "    match = 0\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        flag = 0\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            if y_hat[i,j] != y[i,j]:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            match += 1\n",
    "    return match/y_hat.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "class MNIST_Cnn:\n",
    "    @staticmethod\n",
    "    def modify_label(y):\n",
    "        out = np.zeros((10, y.shape[0]))\n",
    "        for i in range(y.shape[0]):\n",
    "            out[y[i,0], i] = 1\n",
    "        return out\n",
    "\n",
    "\n",
    "    def run_mnist(self):\n",
    "        f1 = open(\"architecture.txt\", \"r\")\n",
    "        lines = f1.readlines()\n",
    "        cnn_layers = list()\n",
    "        for line in lines:\n",
    "            words = line.strip().split()\n",
    "            if words[0].lower() == \"fc\":\n",
    "                cnn_layers.append(FullyConnected(int(words[1])))\n",
    "            elif words[0].lower() == \"relu\":\n",
    "                cnn_layers.append(ReLU())\n",
    "            elif words[0].lower() == \"softmax\":\n",
    "                cnn_layers.append(SoftMax())\n",
    "            elif words[0].lower() == \"conv\":\n",
    "                cnn_layers.append(Convolutional( int(words[1]), int(words[2]), int(words[3]), int(words[4])))\n",
    "            elif words[0].lower() == \"pool\":\n",
    "                cnn_layers.append(MaxPool(int(words[1]), int(words[2])))\n",
    "            elif words[0].lower() == \"flatten\":\n",
    "                cnn_layers.append(Flattening())\n",
    "\n",
    "        f1.close()\n",
    "\n",
    "        x_mnist_train, y_mnist_train = read_mnist_train()\n",
    "        x_mnist_val_test, y_mnist_val_test = read_mnist_test()\n",
    "\n",
    "        split_point = int(y_mnist_val_test.shape[0]/2)\n",
    "\n",
    "        x_mnist_val = x_mnist_val_test[0:split_point,:]\n",
    "        y_mnist_val = y_mnist_val_test[0:split_point]\n",
    "        y_mnist_val = y_mnist_val.reshape(split_point,1)\n",
    "\n",
    "        x_mnist_test = x_mnist_val_test[split_point:,:]\n",
    "        y_mnist_test = y_mnist_val_test[split_point:]\n",
    "        y_mnist_test = y_mnist_test.reshape(split_point,1)\n",
    "\n",
    "        batch_sz = 50\n",
    "\n",
    "        epoch_lim = 5\n",
    "        itr_outer = 1\n",
    "\n",
    "        while itr_outer <= epoch_lim:\n",
    "            print(f'epoch_num: {itr_outer}')\n",
    "            itr_inner = 0\n",
    "            for i in range(0,x_mnist_train.shape[0],batch_sz):\n",
    "                curr_batch_x = x_mnist_train[i:i+batch_sz,:]\n",
    "                curr_batch_y = y_mnist_train[i:i+batch_sz]\n",
    "\n",
    "                curr_batch_x = curr_batch_x.reshape((batch_sz, 28, 28, 1))\n",
    "                curr_batch_y = curr_batch_y.reshape(batch_sz, 1)\n",
    "\n",
    "                curr_batch_y = self.modify_label(curr_batch_y)\n",
    "\n",
    "                prev_a = curr_batch_x\n",
    "                for layer in cnn_layers:\n",
    "                    prev_a = layer.forward(prev_a)\n",
    "\n",
    "                prev_derivative = curr_batch_y\n",
    "                for j in range(len(cnn_layers)-1,0,-1):\n",
    "                    prev_derivative = cnn_layers[j].backward(prev_derivative)\n",
    "\n",
    "\n",
    "                if itr_inner % 10 == 0:\n",
    "                    print(cross_entropy(prev_a, curr_batch_y))\n",
    "\n",
    "                itr_inner += 1\n",
    "\n",
    "\n",
    "            val_x = x_mnist_val.reshape((split_point, 28, 28, 1))\n",
    "            val_y = y_mnist_val.reshape(split_point, 1)\n",
    "\n",
    "            val_y = self.modify_label(val_y)\n",
    "\n",
    "            prev_a = val_x\n",
    "            for layer in cnn_layers:\n",
    "                prev_a = layer.forward(prev_a)\n",
    "\n",
    "            y_pred = np.argmax(prev_a, axis=0)\n",
    "\n",
    "\n",
    "            print(f'f1-score: {f1_score(y_mnist_val, y_pred, average=\"micro\")}')\n",
    "            print(f'accuracy: {accuracy_score(y_mnist_val, y_pred)}')\n",
    "            print(f'loss: {cross_entropy(prev_a, val_y)}')\n",
    "\n",
    "            itr_outer += 1\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run MNIST\n",
    "\"\"\"\n",
    "mnist_Cnn = MNIST_Cnn()\n",
    "mnist_Cnn.run_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR_Cnn:\n",
    "    @staticmethod\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dct = pickle.load(fo, encoding='bytes')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def load_train(self):\n",
    "        x = None\n",
    "        y = None\n",
    "        for i in range(1,6):\n",
    "            file_name = \"cifar/data_batch_\" + str(i)\n",
    "            dct = self.unpickle(file_name)\n",
    "            n_y = np.array(dct[b'labels']).reshape((10000,1))\n",
    "\n",
    "            if i == 1:\n",
    "                x = dct[b'data']\n",
    "                y = n_y\n",
    "            else:\n",
    "                x = np.concatenate((x, dct[b'data']), axis=0)\n",
    "                y = np.concatenate((y, n_y))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def load_test(self):\n",
    "        file_name = \"cifar/test_batch\"\n",
    "        dct = self.unpickle(file_name)\n",
    "        y = np.array(dct[b'labels']).reshape((10000,1))\n",
    "        x = dct[b'data']\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "        num_features = df.shape[1] - 1\n",
    "        df = pd.get_dummies(df, columns=[4], drop_first=False)\n",
    "        train_dataset = df.to_numpy()\n",
    "        x_train = train_dataset[:,:num_features]\n",
    "        y_train = train_dataset[:,num_features:]\n",
    "\n",
    "        x_train = x_train.T\n",
    "        y_train = y_train.T\n",
    "\n",
    "        # print(x_train.shape)\n",
    "        # print(y_train.shape)\n",
    "        return x_train, y_train\n",
    "\n",
    "\n",
    "    def runcnn(self):\n",
    "        f1 = open(\"architecture.txt\", \"r\")\n",
    "        lines = f1.readlines()\n",
    "        cnn_layers = list()\n",
    "        for line in lines:\n",
    "            words = line.strip().split()\n",
    "            if words[0].lower() == \"fc\":\n",
    "                cnn_layers.append(FullyConnected(int(words[1])))\n",
    "            elif words[0].lower() == \"relu\":\n",
    "                cnn_layers.append(ReLU())\n",
    "            elif words[0].lower() == \"softmax\":\n",
    "                cnn_layers.append(SoftMax())\n",
    "\n",
    "        f1.close()\n",
    "\n",
    "        x, y = self.read_data(\"Toy Dataset/trainNN.txt\")\n",
    "        itr_limit = 10000\n",
    "        for itr in range(itr_limit):\n",
    "            prev_a = x\n",
    "            for layer in cnn_layers:\n",
    "                prev_a = layer.forward(prev_a)\n",
    "\n",
    "            prev_derivative = y\n",
    "            for i in range(len(cnn_layers)-1,0,-1):\n",
    "                prev_derivative = cnn_layers[i].backward(prev_derivative)\n",
    "\n",
    "            if itr % 500 == 0:\n",
    "                print(cross_entropy(prev_a, y))\n",
    "\n",
    "        prev_a = self.encode_level(prev_a)\n",
    "        self.calc_accuracy(prev_a, y)\n",
    "\n",
    "        x_test, y_test = self.read_data(\"Toy Dataset/testNN.txt\")\n",
    "\n",
    "        prev_a = x_test\n",
    "        for itr in range(itr_limit):\n",
    "            prev_a = x\n",
    "            for layer in cnn_layers:\n",
    "                prev_a = layer.forward(prev_a)\n",
    "\n",
    "        prev_a = self.encode_level(prev_a)\n",
    "        self.calc_accuracy(prev_a, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nrun cnn\\n'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run cnn\n",
    "\"\"\"\n",
    "# deepNN = DeepNN()\n",
    "# deepNN.runcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}